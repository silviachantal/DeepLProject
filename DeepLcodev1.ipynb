{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3ZT4LulHJ7C"
      },
      "source": [
        "First I import the packages I need\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6JZlyDkHW2J"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tN4QNifzEa_C"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCUeGSn5EfVj"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1t3eDtQUUuG"
      },
      "source": [
        "Here I define my dependent variables (the predictions of valence & arousal) so Y (https://machinelearningmastery.com/multi-label-classification-with-deep-learning/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9AZtgjUVEzDN"
      },
      "outputs": [],
      "source": [
        "def load_dataframe(filename):\n",
        "  dataframe = pd.read_json(filename)\n",
        "  dataframe = dataframe.T\n",
        "  #print(dataframe.head)\n",
        "  return dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfDK7gelUGsa"
      },
      "outputs": [],
      "source": [
        "def loadgroundtruth(dataframe):\n",
        "  #I first zip values of valence and arousal together \n",
        "  Y = list(zip(dataframe[\"valence\"], dataframe[\"activation\"]))\n",
        "  Y = [ torch.Tensor(datapoint, device=device) for datapoint in Y]\n",
        "  return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ED6FJHmFHop"
      },
      "outputs": [],
      "source": [
        "def load_features(dataframe):\n",
        "  X = dataframe[\"features\"]\n",
        "  X = [torch.Tensor(datapoint, device=device) for datapoint in X]\n",
        "  return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW2no3w6sMzU"
      },
      "outputs": [],
      "source": [
        "train_df = load_dataframe('/content/drive/MyDrive/DeepLProject/train.json')\n",
        "dev_df = load_dataframe('/content/drive/MyDrive/DeepLProject/dev.json')\n",
        "\n",
        "train_X = load_features(train_df)\n",
        "train_Y = loadgroundtruth(train_df)\n",
        "dev_X = load_features(dev_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpDej0pjsr-2"
      },
      "source": [
        "Next I split X and Y into training and testing sets with sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J-XsncTSFFG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2X_iCftdkcL"
      },
      "outputs": [],
      "source": [
        "#max_length = max(len(row) for row in X)\n",
        "#max_cols = max([len(row) for batch in X for row in batch])\n",
        "#max_rows = max([len(batch) for batch in X])\n",
        "#padded = [batch + [[0] * (max_cols)] * (max_rows - len(batch)) for batch in X]\n",
        "#padded = torch.tensor([row + [0] * (max_cols - len(row)) for batch in padded for row in batch])\n",
        "#padded = padded.view(-1, max_rows, max_cols)\n",
        "#all matrices now have same length 1707 like biggest one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQhC2fHYJggt"
      },
      "outputs": [],
      "source": [
        "X = [torch.tensor(i) for i in X]\n",
        "Y = [torch.tensor(i, dtype=float) for i in Y]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JciyKbSwKGd_",
        "outputId": "2b442c81-4795-40fd-b0d6-37410a7889c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[5.5028, 5.3896, 5.8908,  ..., 8.2201, 6.8112, 6.4579],\n",
            "        [4.6536, 6.8838, 5.5860,  ..., 7.7556, 6.9207, 6.1452],\n",
            "        [5.0233, 4.1029, 6.8853,  ..., 7.4144, 7.2947, 7.0197],\n",
            "        ...,\n",
            "        [5.7946, 4.0957, 5.4560,  ..., 8.0678, 6.3310, 6.3012],\n",
            "        [6.0561, 6.9045, 7.7520,  ..., 8.1527, 6.9558, 6.7917],\n",
            "        [2.3150, 1.9837, 1.4823,  ..., 2.9754, 1.7627, 2.2844]])\n",
            "torch.Size([147, 26])\n"
          ]
        }
      ],
      "source": [
        "print(X[0]) #list of tensors\n",
        "print(X[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAgHu7ttXpRy",
        "outputId": "12fe0cd0-34c7-4ecc-9aab-4cba258ab7f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0., 1.], dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "print(Y[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWrDeIbiSxRV"
      },
      "outputs": [],
      "source": [
        "X_train, X_dev, Y_train, Y_dev = train_test_split(X, Y, test_size=0.33, random_state=0)\n",
        "#random state is a hyperparameter that controls the shuffling process. With random state set to zero we get the same train and \n",
        "#test sets across different executions \n",
        "#in this case the test set is the dev set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbmMFbKGbENt"
      },
      "outputs": [],
      "source": [
        "#https://colab.research.google.com/drive/1DtQzLzwg9oXm_TCZeG9FJsOxdzE2wM3T?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TU8Ni_lidncM"
      },
      "outputs": [],
      "source": [
        "#I create a class\n",
        "class RNN(nn.Module):\n",
        "  #define the constructor\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes) -> None:\n",
        "        super(RNN,self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        #self.seq_length = seq_length \n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=False)\n",
        "        #This line creates a module for a linear transformation, ùë•ùêñ+ùëèxW+b\n",
        "        #fc means fully connected \n",
        "        self.fc = nn.Linear(hidden_size, num_classes) #(hidden_size*2, num_classes) if i wanted bidirection\n",
        "\n",
        "    #PyTorch networks created with nn.Module must have a forward method. It will take in a tensor and pass it\n",
        "    #through the operations that I have defined in the __init__ method \n",
        "    \n",
        "    def forward(self, x):\n",
        "        sequence_length, n_features = x.shape\n",
        "          #forward propagation of the input through LSTM\n",
        "        out, _ = self.lstm(x) #lstm with input, hidden, and internal state\n",
        "        # out = out.reshape(out.shape[0], -1) #reshaping the data for Dense Layer next\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[-1])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjqGAvptO61s"
      },
      "source": [
        "the lstm will only accept one entry of our sequence at a time\n",
        "so we need to iterate voer the first dimension of the data matrix, \n",
        "and pass this to the lstm while updating the, new freshly generated hidden and cell states\n",
        "one application of the lstm gives somthing like, (out, (h1,c1)) = self.lstm(x[0], (h0, c0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDFSJ2o_ZJ0d"
      },
      "source": [
        "sequence_length, n_features = x.shape # the shape gives us the size of each dimension of the matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t78weXolw98"
      },
      "source": [
        "The schedule your learning rate is going to follow is a major hyperparameter that you want to tune. PyTorch provides support for scheduling learning rates with it‚Äôs torch.optim.lr_scheduler module which has a variety of learning rate schedules. The following example demonstrates one such example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEBgwQe7jpFn",
        "outputId": "7348b2e1-b20d-458b-badc-38f9e7dbd104"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (lstm): LSTM(26, 64)\n",
              "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#here I initialize the model and check how the architecture is represented\n",
        "model = RNN(input_size=26,hidden_size=64, num_layers=1, num_classes=2).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqxmMuPkjtrk"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.003)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ctoaMVDYpkhs",
        "outputId": "c59e19c6-2bb0-40a5-8ece-d0121ef29e62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.4456, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4983, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5247, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4152, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5562, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5611, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.8458, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4034, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5762, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5757, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5706, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3868, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5609, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5433, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5131, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4997, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4361, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4468, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3884, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3864, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4667, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4675, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4491, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4428, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4706, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4806, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3931, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4234, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0710, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4196, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4182, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4955, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4123, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4999, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4069, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5045, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4066, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3982, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3938, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.1329, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4128, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.1392, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3868, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3883, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.1379, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3880, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3881, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3871, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3813, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5526, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3617, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3863, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3564, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3486, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3436, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5889, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3339, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3331, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3238, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3174, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3110, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.6465, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3548, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3525, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3411, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.6299, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.2917, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3348, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5769, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3573, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5281, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3913, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3998, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5131, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4093, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4169, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4930, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4187, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4178, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4949, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4944, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4927, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4897, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4353, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4396, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0343, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4414, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0268, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0185, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4603, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4520, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4793, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.9568, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4961, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4265, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4249, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4186, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5382, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5402, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5442, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5343, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5320, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4249, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5095, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5069, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4964, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4855, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4706, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4567, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0221, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4466, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4767, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4322, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0570, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4268, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0661, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4267, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4251, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0913, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4865, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4184, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4269, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4930, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4841, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4294, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4817, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0488, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4761, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4450, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4442, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4507, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4499, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4430, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4370, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4313, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4214, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4958, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5032, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4029, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5163, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5189, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.1343, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5136, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4043, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5053, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4992, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.6709, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4802, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4456, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4578, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4580, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4537, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.9664, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4893, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4350, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4323, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4233, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5271, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.6172, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5334, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4175, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5314, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5263, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5207, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4267, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5057, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5003, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4889, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4799, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4708, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4563, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4837, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4937, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4134, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5011, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4060, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3929, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3955, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3833, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3704, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.1882, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5582, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3650, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5570, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3611, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3591, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.2078, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4794, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3587, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3572, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3948, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3415, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5794, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5784, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5774, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3400, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5717, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3523, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3547, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.2717, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3581, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5615, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3615, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.2351, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3632, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5452, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3730, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5378, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3849, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3895, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5221, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3863, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.1294, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3945, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3961, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3951, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3958, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5184, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3910, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5242, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5249, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0831, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5175, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.1031, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4103, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4245, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4845, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4319, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4359, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4352, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0475, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4775, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4376, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.7125, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4706, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4704, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4520, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4539, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4631, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4536, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0116, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4616, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4586, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4515, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4959, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4453, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4421, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4390, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.9396, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4297, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4242, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4192, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4142, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.8585, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4047, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5815, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4313, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.6256, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3908, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3900, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.6377, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3899, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.6349, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.6307, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3915, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3924, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.6131, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.6025, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3997, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5732, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.8590, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4102, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4091, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4093, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4102, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5525, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5508, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4109, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.8691, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4111, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5539, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4093, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4103, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5534, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.8622, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4097, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5565, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4076, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4073, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5596, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4077, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4077, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5586, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5558, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5478, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5416, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.8994, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5159, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4266, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4296, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4299, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5053, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5013, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4958, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4394, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4441, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4443, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4475, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.2012, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4786, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4483, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4745, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.9854, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4707, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.9866, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4501, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4738, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4773, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4758, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4717, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3937, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4626, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4626, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4662, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0214, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4667, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4503, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0106, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4604, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4633, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.9987, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4632, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4571, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4677, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4602, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4975, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4582, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4595, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3894, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4557, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4494, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3876, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4404, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4388, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4317, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4880, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0763, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4929, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3810, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4161, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4939, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4989, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4186, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4939, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.7422, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4925, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4213, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4227, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4197, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4922, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4122, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4087, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4028, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.1178, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5141, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3949, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.1244, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5135, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4030, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4072, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.1015, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4108, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4973, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.9661, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4883, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4829, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4395, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0252, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5280, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4602, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.9873, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4572, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4806, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4484, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.9597, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4391, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4959, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4325, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4275, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.9048, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.8933, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5442, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5543, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.8468, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5656, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.8330, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.7827, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3896, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.6931, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5708, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.6858, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3895, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.6344, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.2571, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3967, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5871, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5690, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5536, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4935, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4513, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5010, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4185, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4992, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4129, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4041, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4005, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3792, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3669, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5876, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5740, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5773, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.2352, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(2.1306, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3594, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3628, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3767, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3636, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3603, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4032, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5727, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3485, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3445, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5851, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3365, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3329, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.6032, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3266, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3216, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.6216, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.6260, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.6211, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.6009, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5838, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3560, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3668, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3711, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.1730, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.1545, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5190, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4051, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4794, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4682, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4624, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4810, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4736, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4748, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.9315, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4263, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4211, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4075, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4025, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3964, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3944, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.6317, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3902, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.6464, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3891, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.6344, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.8409, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.6261, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.6066, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.3872, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5812, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4048, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5359, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4149, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4239, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5144, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5104, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4494, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4775, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4642, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4554, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0631, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4861, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4839, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4346, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0406, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4546, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4593, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.9909, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4777, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4854, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4871, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4794, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4720, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4560, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4573, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4664, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4435, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0429, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4770, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4369, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4372, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4320, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4876, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4223, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4966, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5045, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4063, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4026, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3983, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(-0., dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3842, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5401, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.1762, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.3726, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5417, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.5380, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.1795, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.1254, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.4096, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4884, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4754, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0084, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.0008, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(1.4321, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.8919, dtype=torch.float64, grad_fn=<DivBackward1>)\n",
            "tensor(0.5588, dtype=torch.float64, grad_fn=<DivBackward1>)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-2f146b791e00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Training the network\n",
        "for epoch in range(1000):\n",
        "    for x, y in zip(X_train, Y_train):\n",
        "        \n",
        "        #Get data to cuda\n",
        "        x = x.to(device=device)\n",
        "        y = y.to(device=device)\n",
        "\n",
        "        #forward\n",
        "        scores = model(x)\n",
        "        loss = criterion(scores, y)\n",
        "        print(loss)\n",
        "        \n",
        "        #backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        #gradient descent\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiyoAk2iznOa"
      },
      "outputs": [],
      "source": [
        "\n",
        "def check_accuracy(loader, model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x,y in loader:\n",
        "            x = x.to(device=device).squeeze(1)\n",
        "            y = y.to(device=device)\n",
        "            # x = x.reshape(x.shape[0], -1)\n",
        "\n",
        "            scores = model(x)\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
        "    model.train()\n",
        "    \n",
        "\n",
        "check_accuracy(train_loader, model)\n",
        "check_accuracy(test_loader, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RilxxMKkZxBR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}